apiVersion: batch/v1
kind: Job
metadata:
  name: {{ include "falco-helpers.updateLogic" . }}
  namespace: {{ .Release.Namespace | quote }}
  annotations:
    # create hook dependencies in the right order
    "helm.sh/hook-weight": "-1"
    {{- include "falco-helpers.UpdateLogicAnnotations" . | nindent 4 }}
  labels:
    app.kubernetes.io/component: {{ include "falco-helpers.updateLogic" . | quote }}
    {{- include "falco.selectorLabels" . | nindent 4 }}
    role: {{ include "falco-helpers.UpdateLogicSelector" . | quote }}
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/component: {{ include "falco-helpers.updateLogic" . | quote }}
        {{- include "falco.selectorLabels" . | nindent 8 }}
    spec:
      serviceAccountName: {{ include "falco-helpers.updateLogic" . }}
      securityContext:
        runAsUser: 65534
        runAsGroup: 65534
        {{- with .Values.updateLogic.podSecurityContext }}
        {{- . | toYaml | nindent 8 }}
        {{- end }}
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: kubectl
        image: "{{ .Values.updateLogic.image.registry }}/giantswarm/docker-kubectl:{{ .Values.updateLogic.image.tag }}"
        command:
        - sh
        - -c
        - |
          set -o errexit ; set -o xtrace ; set -o nounset

          # piping stderr to stdout means kubectl's errors are surfaced
          # in the pod's logs.

          # Workaround for broken upgrade path going to 0.33.0.
          kubectl delete ds -n $pod_namespace {{ include "falco.fullname" . }}

        env:
          - name: pod_namespace
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
        resources: {{- toYaml .Values.updateLogic.resources | nindent 10 }}
        {{- with .Values.updateLogic.securityContext }}
        securityContext:
          {{- . | toYaml | nindent 10 }}
        {{- end }}
      restartPolicy: Never
  backoffLimit: 4
